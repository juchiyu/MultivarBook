<!DOCTYPE html>
<html >

<head>

  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>1 What is Principal Component Analysis (PCA)? | Multivariate Cookbook</title>
  <meta name="description" content="This book provides an introduction of simple and advanced multivariate analysis.">
  <meta name="generator" content="bookdown  and GitBook 2.6.7">

  <meta property="og:title" content="1 What is Principal Component Analysis (PCA)? | Multivariate Cookbook" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="This book provides an introduction of simple and advanced multivariate analysis." />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="1 What is Principal Component Analysis (PCA)? | Multivariate Cookbook" />
  
  <meta name="twitter:description" content="This book provides an introduction of simple and advanced multivariate analysis." />
  

<meta name="author" content="Ju-Chi Yu">


<meta name="date" content="2019-11-16">

  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  
  
<link rel="prev" href="pca.html">
<link rel="next" href="svd.html">
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />









<style type="text/css">
a.sourceLine { display: inline-block; line-height: 1.25; }
a.sourceLine { pointer-events: none; color: inherit; text-decoration: inherit; }
a.sourceLine:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
a.sourceLine { text-indent: -1em; padding-left: 1em; }
}
pre.numberSource a.sourceLine
  { position: relative; left: -4em; }
pre.numberSource a.sourceLine::before
  { content: attr(title);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; pointer-events: all; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {  }
@media screen {
a.sourceLine::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li class="chapter" data-level="" data-path="preface.html"><a href="preface.html"><i class="fa fa-check"></i>Preface</a><ul>
<li class="chapter" data-level="0.1" data-path="preface.html"><a href="preface.html#what-is-multivariate-analysis"><i class="fa fa-check"></i><b>0.1</b> What is multivariate analysis?</a></li>
<li class="chapter" data-level="0.2" data-path="preface.html"><a href="preface.html#common-methods-in-multivariate-analysis"><i class="fa fa-check"></i><b>0.2</b> Common methods in multivariate analysis</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="pca.html"><a href="pca.html"><i class="fa fa-check"></i>PCA</a></li>
<li class="chapter" data-level="1" data-path="pca-intro.html"><a href="pca-intro.html"><i class="fa fa-check"></i><b>1</b> What is Principal Component Analysis (PCA)?</a></li>
<li class="chapter" data-level="2" data-path="svd.html"><a href="svd.html"><i class="fa fa-check"></i><b>2</b> Singular value decomposition</a></li>
<li class="chapter" data-level="3" data-path="pca-example.html"><a href="pca-example.html"><i class="fa fa-check"></i><b>3</b> PCA example</a><ul>
<li class="chapter" data-level="3.1" data-path="pca-example.html"><a href="pca-example.html#example-data"><i class="fa fa-check"></i><b>3.1</b> Example data</a><ul>
<li class="chapter" data-level="3.1.1" data-path="pca-example.html"><a href="pca-example.html#design-of-the-data"><i class="fa fa-check"></i><b>3.1.1</b> Design of the data</a></li>
</ul></li>
</ul></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Multivariate Cookbook</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="pca_intro" class="section level1">
<h1><span class="header-section-number">1</span> What is Principal Component Analysis (PCA)?</h1>

<p>Multi-table techniques originate from the multivariate techniques that analyze a single table (i.e., PCA) and a pair of tables (i.e., PLSC, CCA and RDA). These techniques search for a linear combination (i.e., a mixture) of the original variables that gives the maximum amount of information. However, the information to be maximized is defined differently for different techniques, and, therefore, these techniques can be expressed as different optimization problems, which can usually be solved by a gradient descent algorithm that iteratively searches for the solution (i.e., the coefficients of this linear combination). Specifically, in each , the gradient descent algorithm generates a new set of numbers that gives more information about the data than the previously generated set of numbers, and this procedure repeats for several iterations until an optimum is reached. Interestingly, for most multivariate techniques, this optimum set of coefficients can also be found in one step by the SVD.</p>

<p>The SVD is a maximization problem (i.e., an optimization problem that searches for a maximum) that works on an <span class="math inline">\(\times\)</span> rectangular matrix . The SVD searches for two vectors of coefficients denoted  and  which maximize the  <span class="math inline">\(\delta\)</span> computed as:
<span class="math display">\[\begin{equation}\label{Maximization_SVD}
\delta = \argmax_{\mathbf{p},\mathbf{q}} \left\{\mathbf{p\transpose Xq}\right\}\quad\mathrm{under\;the\;constraints}\quad\mathbf{p\transpose p = q\transpose q =\:}1.
\end{equation}\]</span>
Here,  (respectively ) is the  (respectively )  that stores the coefficients to weight the rows (respectively the columns) of .</p>
<p>This maximization problem can be solved by a gradient descent algorithm which consists of the following steps. First, a set of values is used to initialize the first left singular vector <span class="math inline">\(\prescript{}{}{\mathbf{p}}^1\)</span>, and these initialized (denoted by the subscript 0 on the left) values <span class="math inline">\(\prescript{}{0}{\mathbf{p}}^1\)</span> are then used to compute the initial values of $ ^1 $ (denoted by <span class="math inline">\(\prescript{}{0}{\mathbf{q}}^1\)</span>):
<span class="math display">\[\begin{equation}\label{iter1_SVD}
\delta\prescript{}{0}{\mathbf{q}}^1\mathbf{= X\transpose}\prescript{}{0}{\mathbf{p}}^1.
\end{equation}\]</span>
This $^1 $ can then be used to compute <span class="math inline">\(\prescript{}{1}{\mathbf{p}}^1\)</span>:
<span class="math display">\[\begin{equation}\label{iter2_SVD}
\delta\prescript{}{1}{\mathbf{p}}^1\mathbf{= X}\prescript{}{0}{\mathbf{q}}^1.
\end{equation}\]</span>
Next, Equations  and  are repeated respectively with <span class="math inline">\(\prescript{}{1}{\mathbf{p}}^1\)</span> and the next derived <span class="math inline">\(\prescript{}{1}{\mathbf{q}}^1\)</span>, and this procedure is iterated until $^1 $ and $ ^1 $ converge to stable outcomes. After the first set of singular vectors has been extracted, the second set (<span class="math inline">\(\textbf{p}^2\)</span> and <span class="math inline">\(\textbf{q}^2\)</span>) is derived from the data that are left (i.e., the ; <span class="math inline">\(\textbf{X} - \delta\mathbf{p}^1{\mathbf{q}^1}\transpose\)</span>) and is thus  to the first set of singular vectors (i.e., $ {<sup>1}</sup>2 = 0 $). Following the extraction of the second set of singular vectors, the third set can be extracted from the residual of the first two singular vectors and so on. As a result, together with the constraints from Equation , the relationship between different sets of singular vectors can be written as:
<span class="math display">\[\begin{equation}\label{MaximizationConstraints_SVD}
{\mathbf{p}^\ell}\transpose \mathbf{p}^{\ell^\prime} = {\mathbf{q}^\ell}\transpose \mathbf{q}^{\ell^\prime} =
\begin{cases}
\quad 1, &amp; \mathrm{when}\;\ell = \ell^\prime\\
\quad 0, &amp; \mathrm{when}\;\ell\neq\ell^\prime
\end{cases}.
\end{equation}\]</span>
If these multiple sets of singular vectors <span class="math inline">\(\textbf{p}^\ell\)</span> and <span class="math inline">\(\textbf{q}^\ell\)</span> are stored in the columns of matrices  and :
<span class="math display">\[\begin{equation}\label{buildPQ_SVD}
\mathbf{P} = \left[\mathbf{p}^1\,|\,\mathbf{p}^2\,|\,...\,|\,\mathbf{p}^\ell|\,...\,|\,\mathbf{p}^\textit{L}\right]
\quad\mathrm{and}\quad
\mathbf{Q} = \left[\mathbf{q}^1\,|\,\mathbf{q}^2\,|\,...\,|\,\mathbf{q}^\ell|\,...\,|\,\mathbf{q}^\textit{L}\right],
\end{equation}\]</span>
then SVD (cf. Equation ) can be expressed as:
<span class="math display">\[\begin{equation}
\mathbf{X = P\Delta Q\transpose}
\label{PDQ}
\end{equation}\]</span>
with the constraints rewritten from Equation  as:
<span class="math display">\[\begin{equation}
\textbf{P}\transpose\textbf{P} = \textbf{Q}\transpose\textbf{Q} = \textbf{I}.
\label{PTPQTQequalI}
\end{equation}\]</span></p>
<p>Here,  is the <span class="math inline">\(\times\)</span> matrix of the left singular vectors,  is the <span class="math inline">\(\times\)</span> matrix of the right singular vectors, and <span class="math inline">\(\mathbf{\Delta}\)</span> is the the <span class="math inline">\(\times\)</span> diagonal matrix of the singular values <span class="math inline">\(\delta\)</span>s. In addition, these $ $s are ordered from the largest to the smallest (i.e., <span class="math inline">\(\delta_1 &gt; \delta_2 &gt;...&gt; \delta_\textit{L}\)</span>), and the total number of non-zero singular values is equal to the rank of . Thus, the SVD gives the optimal decomposition of  with a given rank.</p>
<p>Also, the squared singular values <span class="math inline">\(\mathbf{\delta}^2\)</span> (or <span class="math inline">\(\mathbf{\Delta}^2\)</span> as a diagonal matrix) are equal to the  <span class="math inline">\(\mathbf{\lambda}\)</span> (or <span class="math inline">\(\mathbf{\Lambda}\)</span> as a diagonal matrix) of matrices <span class="math inline">\(\transpose\)</span> and <span class="math inline">\(\transpose\)</span>, and  and  are the respective  of <span class="math inline">\(\transpose\)</span> and <span class="math inline">\(\transpose\)</span>. Note that when the eigenvalues of a matrix (e.g., ) are all positive, this matrix is called  (pd); when the eigenvalues of this matrix are non-negative values, this matrix is called  (psd).</p>

<p>PCA extracts information by generating a set of orthogonal variables, called , with the first component explaining most of the variance of the data table. The elements of these components are called  or . First, almost always in PCA, the data table is preprocessed by centering its columns (i.e., each column has a mean of zero). Next, the columns can be normalized to exclude their units and are therefore comparable to each other. Finally, the set of factor scores of the first component ($ ^1 $) is computed as a linear combination of the original variables:
<span class="math display">\[\begin{equation}\label{f1_PCA}
\mathbf{f}^1 = \mathbf{Xq}^1,
\end{equation}\]</span>
where $ ^1 $ is a  $ $ 1 right singular vector that stores the coefficients for each column of , and in PCA, these coefficients are called . Furthermore, because the first component is defined to have the largest variance, and because its variance is computed by the sum of squares of its factor scores:
<span class="math display">\[\begin{equation}\label{f1var_PCA}
{\mathbf{f}^1}\transpose\mathbf{f}^1 = {\mathbf{q}^1}\transpose\mathbf{X\transpose} \mathbf{Xq}^1,
\end{equation}\]</span>
the step to find $ ^1 $ can then be described as the solution of the following maximization problem:
<span class="math display">\[\begin{equation}\label{Maximization_PCA}
\begin{split}
\delta^2 &amp;= \argmax_{\mathbf{q}^1} \left\{{\mathbf{f}^1}\transpose\mathbf{f}^1\right\}
= \argmax_{\mathbf{q}^1} \left\{{\mathbf{q}^1}\transpose\mathbf{X\transpose} \mathbf{Xq}^1\right\} \\
&amp;= \argmax_{\mathbf{q}^1}\left\{\mathrm{var}(\mathbf{Xq}^1)\right\} = \argmax_{\mathbf{q}^1} \left\{\mathrm{cov}(\mathbf{Xq}^1, \mathbf{Xq}^1)\right\}\quad
\mathrm{where}\quad{\mathbf{q}^1}\transpose \mathbf{q}^1 = 1. \\
\end{split}
\end{equation}\]</span>
After the set of factor scores of the first component have been found, the set of factor scores of the following components are computed sequentially from the residual so that these components are orthogonal.</p>
<p>If the factor scores $ ^$ and loadings $ ^$ of each component are stored in different columns of matrices  and  such that
<span class="math display">\[\begin{equation}\label{buildFQ_PCA}
\mathbf{F} = \left[\mathbf{f}^1\,|\,\mathbf{f}^2\,|\,...\,|\,\mathbf{f}^\ell|\,...\,|\,\mathbf{f}^\textit{L}\right]
\quad\mathrm{and}\quad
\mathbf{Q} = \left[\mathbf{q}^1\,|\,\mathbf{q}^2\,|\,...\,|\,\mathbf{q}^\ell|\,...\,|\,\mathbf{q}^\textit{L}\right],
\end{equation}\]</span>
Equation  can be rewritten as:
<span class="math display">\[\begin{equation}
\mathbf{F = XQ}
\quad\mathrm{under\;the\;constraint\;that}\quad
\mathbf{Q\transpose Q = I}.
\label{FactorScores}
\end{equation}\]</span>
By computing the factor scores, PCA   onto the components; in other words, the factor scores $ ^$ are the  of the rows of  onto the $ $th component.</p>
<p>In PCA, the maximization problem can also be rewritten from Equation  as the SVD of  (recall Equation ):
<span class="math display">\[\begin{equation}
\mathbf{X = FQ\transpose = P\Delta Q\transpose}\quad\mathrm{under\;the\;constraints}\quad
\mathbf{P\transpose P = Q\transpose Q = I}.
\label{XequalFQ}
\end{equation}\]</span>
In Equations  and ,  (respectively ) is the  (respectively ) <span class="math inline">\(\times\)</span> matrix of the left (respectively right) singular vectors, and <span class="math inline">\(\mathbf{\Delta}\)</span> is the <span class="math inline">\(\times\)</span> diagonal matrix of the singular values <span class="math inline">\(\delta\)</span>s. Moreover, if we rewrite Equation  from Equation , the factor scores can also be computed from  and <span class="math inline">\(\boldsymbol{\Delta}\)</span> as:
<span class="math display">\[\begin{equation}
\mathbf{F = XQ = P\Delta}
\quad\mathrm{under\;the\;constraints}\quad
\mathbf{P\transpose P = Q\transpose Q = I},
\label{FactorScores2}
\end{equation}\]</span>
and the sums of squares of  that are maximized in PCA are equal to the eigenvalues <span class="math inline">\(\mathbf{\Lambda}\)</span> because
<span class="math display">\[\begin{equation}\label{FtFeqLambda}
\mathbf{F\transpose F = \Delta P\transpose P\Delta = \Delta}^2 = \mathbf{\Lambda}. 
\end{equation}\]</span>
As a result, each eigenvalue is the sum of squares of each component, and the sum of all eigenvalues gives the total sum of squares of the table—a quantity called the  of the data table. When the eigenvalues are divided by <span class="math inline">\(\textit{I}-1\)</span>, they give the variance of the components. Because the singular values are the square roots of the eigenvalues, singular values divided by <span class="math inline">\(\sqrt[]{\textit{I}-1}\)</span> give the standard deviation of the components.</p>
<p>In addition, Equation  can be applied to map an out-of-sample observation (e.g., <span class="math inline">\(\mathbf{r}_\mathrm{sup}\)</span> as a row with matching columns) or an out-of-sample variable (e.g., <span class="math inline">\(\mathbf{c}_\mathrm{sup}\)</span> as a column with matching rows) onto the components. These out-of-sample elements are called  elements—as opposed to the  elements that are originally analyzed by PCA—and need to be preprocessed (i.e., centered and normalized) in the same way as the active elements of the original PCA. After proper preprocessing steps, the supplementary factor scores and loadings are computed as
<span class="math display">\[\begin{equation}
\mathbf{f}_\mathrm{sup} = \mathbf{r}_\mathrm{sup} \mathbf{Q}\qquad
\mathrm{and}\qquad
\mathbf{q}_\mathrm{sup} = \mathbf{c}_\mathrm{sup} \mathbf{P\Delta}^{-1}.
\label{supp_PCA}
\end{equation}\]</span></p>

</div>
            </section>

          </div>
        </div>
      </div>
<a href="pca.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="svd.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"google": false,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/juchiyu/MultivarBook/edit/master/Rmd/01_Ch1.1_PCAintro.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"download": null,
"toc": {
"collapse": "section",
"scroll_highlight": true
},
"toolbar": {
"position": "static"
},
"search": true
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:" && /^https?:/.test(src))
      src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
