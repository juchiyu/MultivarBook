[
["preface.html", "Multivariate Cookbook Preface 0.1 What is multivariate analysis? 0.2 Common methods in multivariate analysis", " Multivariate Cookbook Ju-Chi Yu 2019-11-16 Preface 0.1 What is multivariate analysis? 0.2 Common methods in multivariate analysis "],
["pca.html", "PCA", " PCA "],
["pca-intro.html", "1 What is Principal Component Analysis (PCA)?", " 1 What is Principal Component Analysis (PCA)? Multi-table techniques originate from the multivariate techniques that analyze a single table (i.e., PCA) and a pair of tables (i.e., PLSC, CCA and RDA). These techniques search for a linear combination (i.e., a mixture) of the original variables that gives the maximum amount of information. However, the information to be maximized is defined differently for different techniques, and, therefore, these techniques can be expressed as different optimization problems, which can usually be solved by a gradient descent algorithm that iteratively searches for the solution (i.e., the coefficients of this linear combination). Specifically, in each , the gradient descent algorithm generates a new set of numbers that gives more information about the data than the previously generated set of numbers, and this procedure repeats for several iterations until an optimum is reached. Interestingly, for most multivariate techniques, this optimum set of coefficients can also be found in one step by the SVD. The SVD is a maximization problem (i.e., an optimization problem that searches for a maximum) that works on an \\(\\times\\) rectangular matrix . The SVD searches for two vectors of coefficients denoted and which maximize the \\(\\delta\\) computed as: \\[\\begin{equation}\\label{Maximization_SVD} \\delta = \\argmax_{\\mathbf{p},\\mathbf{q}} \\left\\{\\mathbf{p\\transpose Xq}\\right\\}\\quad\\mathrm{under\\;the\\;constraints}\\quad\\mathbf{p\\transpose p = q\\transpose q =\\:}1. \\end{equation}\\] Here, (respectively ) is the (respectively ) that stores the coefficients to weight the rows (respectively the columns) of . This maximization problem can be solved by a gradient descent algorithm which consists of the following steps. First, a set of values is used to initialize the first left singular vector \\(\\prescript{}{}{\\mathbf{p}}^1\\), and these initialized (denoted by the subscript 0 on the left) values \\(\\prescript{}{0}{\\mathbf{p}}^1\\) are then used to compute the initial values of $ ^1 $ (denoted by \\(\\prescript{}{0}{\\mathbf{q}}^1\\)): \\[\\begin{equation}\\label{iter1_SVD} \\delta\\prescript{}{0}{\\mathbf{q}}^1\\mathbf{= X\\transpose}\\prescript{}{0}{\\mathbf{p}}^1. \\end{equation}\\] This $^1 $ can then be used to compute \\(\\prescript{}{1}{\\mathbf{p}}^1\\): \\[\\begin{equation}\\label{iter2_SVD} \\delta\\prescript{}{1}{\\mathbf{p}}^1\\mathbf{= X}\\prescript{}{0}{\\mathbf{q}}^1. \\end{equation}\\] Next, Equations and are repeated respectively with \\(\\prescript{}{1}{\\mathbf{p}}^1\\) and the next derived \\(\\prescript{}{1}{\\mathbf{q}}^1\\), and this procedure is iterated until $^1 $ and $ ^1 $ converge to stable outcomes. After the first set of singular vectors has been extracted, the second set (\\(\\textbf{p}^2\\) and \\(\\textbf{q}^2\\)) is derived from the data that are left (i.e., the ; \\(\\textbf{X} - \\delta\\mathbf{p}^1{\\mathbf{q}^1}\\transpose\\)) and is thus to the first set of singular vectors (i.e., $ {1}2 = 0 $). Following the extraction of the second set of singular vectors, the third set can be extracted from the residual of the first two singular vectors and so on. As a result, together with the constraints from Equation , the relationship between different sets of singular vectors can be written as: \\[\\begin{equation}\\label{MaximizationConstraints_SVD} {\\mathbf{p}^\\ell}\\transpose \\mathbf{p}^{\\ell^\\prime} = {\\mathbf{q}^\\ell}\\transpose \\mathbf{q}^{\\ell^\\prime} = \\begin{cases} \\quad 1, &amp; \\mathrm{when}\\;\\ell = \\ell^\\prime\\\\ \\quad 0, &amp; \\mathrm{when}\\;\\ell\\neq\\ell^\\prime \\end{cases}. \\end{equation}\\] If these multiple sets of singular vectors \\(\\textbf{p}^\\ell\\) and \\(\\textbf{q}^\\ell\\) are stored in the columns of matrices and : \\[\\begin{equation}\\label{buildPQ_SVD} \\mathbf{P} = \\left[\\mathbf{p}^1\\,|\\,\\mathbf{p}^2\\,|\\,...\\,|\\,\\mathbf{p}^\\ell|\\,...\\,|\\,\\mathbf{p}^\\textit{L}\\right] \\quad\\mathrm{and}\\quad \\mathbf{Q} = \\left[\\mathbf{q}^1\\,|\\,\\mathbf{q}^2\\,|\\,...\\,|\\,\\mathbf{q}^\\ell|\\,...\\,|\\,\\mathbf{q}^\\textit{L}\\right], \\end{equation}\\] then SVD (cf. Equation ) can be expressed as: \\[\\begin{equation} \\mathbf{X = P\\Delta Q\\transpose} \\label{PDQ} \\end{equation}\\] with the constraints rewritten from Equation as: \\[\\begin{equation} \\textbf{P}\\transpose\\textbf{P} = \\textbf{Q}\\transpose\\textbf{Q} = \\textbf{I}. \\label{PTPQTQequalI} \\end{equation}\\] Here, is the \\(\\times\\) matrix of the left singular vectors, is the \\(\\times\\) matrix of the right singular vectors, and \\(\\mathbf{\\Delta}\\) is the the \\(\\times\\) diagonal matrix of the singular values \\(\\delta\\)s. In addition, these $ $s are ordered from the largest to the smallest (i.e., \\(\\delta_1 &gt; \\delta_2 &gt;...&gt; \\delta_\\textit{L}\\)), and the total number of non-zero singular values is equal to the rank of . Thus, the SVD gives the optimal decomposition of with a given rank. Also, the squared singular values \\(\\mathbf{\\delta}^2\\) (or \\(\\mathbf{\\Delta}^2\\) as a diagonal matrix) are equal to the \\(\\mathbf{\\lambda}\\) (or \\(\\mathbf{\\Lambda}\\) as a diagonal matrix) of matrices \\(\\transpose\\) and \\(\\transpose\\), and and are the respective of \\(\\transpose\\) and \\(\\transpose\\). Note that when the eigenvalues of a matrix (e.g., ) are all positive, this matrix is called (pd); when the eigenvalues of this matrix are non-negative values, this matrix is called (psd). PCA extracts information by generating a set of orthogonal variables, called , with the first component explaining most of the variance of the data table. The elements of these components are called or . First, almost always in PCA, the data table is preprocessed by centering its columns (i.e., each column has a mean of zero). Next, the columns can be normalized to exclude their units and are therefore comparable to each other. Finally, the set of factor scores of the first component ($ ^1 $) is computed as a linear combination of the original variables: \\[\\begin{equation}\\label{f1_PCA} \\mathbf{f}^1 = \\mathbf{Xq}^1, \\end{equation}\\] where $ ^1 $ is a $ $ 1 right singular vector that stores the coefficients for each column of , and in PCA, these coefficients are called . Furthermore, because the first component is defined to have the largest variance, and because its variance is computed by the sum of squares of its factor scores: \\[\\begin{equation}\\label{f1var_PCA} {\\mathbf{f}^1}\\transpose\\mathbf{f}^1 = {\\mathbf{q}^1}\\transpose\\mathbf{X\\transpose} \\mathbf{Xq}^1, \\end{equation}\\] the step to find $ ^1 $ can then be described as the solution of the following maximization problem: \\[\\begin{equation}\\label{Maximization_PCA} \\begin{split} \\delta^2 &amp;= \\argmax_{\\mathbf{q}^1} \\left\\{{\\mathbf{f}^1}\\transpose\\mathbf{f}^1\\right\\} = \\argmax_{\\mathbf{q}^1} \\left\\{{\\mathbf{q}^1}\\transpose\\mathbf{X\\transpose} \\mathbf{Xq}^1\\right\\} \\\\ &amp;= \\argmax_{\\mathbf{q}^1}\\left\\{\\mathrm{var}(\\mathbf{Xq}^1)\\right\\} = \\argmax_{\\mathbf{q}^1} \\left\\{\\mathrm{cov}(\\mathbf{Xq}^1, \\mathbf{Xq}^1)\\right\\}\\quad \\mathrm{where}\\quad{\\mathbf{q}^1}\\transpose \\mathbf{q}^1 = 1. \\\\ \\end{split} \\end{equation}\\] After the set of factor scores of the first component have been found, the set of factor scores of the following components are computed sequentially from the residual so that these components are orthogonal. If the factor scores $ ^$ and loadings $ ^$ of each component are stored in different columns of matrices and such that \\[\\begin{equation}\\label{buildFQ_PCA} \\mathbf{F} = \\left[\\mathbf{f}^1\\,|\\,\\mathbf{f}^2\\,|\\,...\\,|\\,\\mathbf{f}^\\ell|\\,...\\,|\\,\\mathbf{f}^\\textit{L}\\right] \\quad\\mathrm{and}\\quad \\mathbf{Q} = \\left[\\mathbf{q}^1\\,|\\,\\mathbf{q}^2\\,|\\,...\\,|\\,\\mathbf{q}^\\ell|\\,...\\,|\\,\\mathbf{q}^\\textit{L}\\right], \\end{equation}\\] Equation can be rewritten as: \\[\\begin{equation} \\mathbf{F = XQ} \\quad\\mathrm{under\\;the\\;constraint\\;that}\\quad \\mathbf{Q\\transpose Q = I}. \\label{FactorScores} \\end{equation}\\] By computing the factor scores, PCA onto the components; in other words, the factor scores $ ^$ are the of the rows of onto the $ $th component. In PCA, the maximization problem can also be rewritten from Equation as the SVD of (recall Equation ): \\[\\begin{equation} \\mathbf{X = FQ\\transpose = P\\Delta Q\\transpose}\\quad\\mathrm{under\\;the\\;constraints}\\quad \\mathbf{P\\transpose P = Q\\transpose Q = I}. \\label{XequalFQ} \\end{equation}\\] In Equations and , (respectively ) is the (respectively ) \\(\\times\\) matrix of the left (respectively right) singular vectors, and \\(\\mathbf{\\Delta}\\) is the \\(\\times\\) diagonal matrix of the singular values \\(\\delta\\)s. Moreover, if we rewrite Equation from Equation , the factor scores can also be computed from and \\(\\boldsymbol{\\Delta}\\) as: \\[\\begin{equation} \\mathbf{F = XQ = P\\Delta} \\quad\\mathrm{under\\;the\\;constraints}\\quad \\mathbf{P\\transpose P = Q\\transpose Q = I}, \\label{FactorScores2} \\end{equation}\\] and the sums of squares of that are maximized in PCA are equal to the eigenvalues \\(\\mathbf{\\Lambda}\\) because \\[\\begin{equation}\\label{FtFeqLambda} \\mathbf{F\\transpose F = \\Delta P\\transpose P\\Delta = \\Delta}^2 = \\mathbf{\\Lambda}. \\end{equation}\\] As a result, each eigenvalue is the sum of squares of each component, and the sum of all eigenvalues gives the total sum of squares of the table—a quantity called the of the data table. When the eigenvalues are divided by \\(\\textit{I}-1\\), they give the variance of the components. Because the singular values are the square roots of the eigenvalues, singular values divided by \\(\\sqrt[]{\\textit{I}-1}\\) give the standard deviation of the components. In addition, Equation can be applied to map an out-of-sample observation (e.g., \\(\\mathbf{r}_\\mathrm{sup}\\) as a row with matching columns) or an out-of-sample variable (e.g., \\(\\mathbf{c}_\\mathrm{sup}\\) as a column with matching rows) onto the components. These out-of-sample elements are called elements—as opposed to the elements that are originally analyzed by PCA—and need to be preprocessed (i.e., centered and normalized) in the same way as the active elements of the original PCA. After proper preprocessing steps, the supplementary factor scores and loadings are computed as \\[\\begin{equation} \\mathbf{f}_\\mathrm{sup} = \\mathbf{r}_\\mathrm{sup} \\mathbf{Q}\\qquad \\mathrm{and}\\qquad \\mathbf{q}_\\mathrm{sup} = \\mathbf{c}_\\mathrm{sup} \\mathbf{P\\Delta}^{-1}. \\label{supp_PCA} \\end{equation}\\] "],
["svd.html", "2 Singular value decomposition", " 2 Singular value decomposition "],
["pca-example.html", "3 PCA example 3.1 Example data", " 3 PCA example 3.1 Example data library(ExPosition) ## Loading required package: prettyGraphs data(words) words$data ## length definitions ## bag 3 14 ## across 6 7 ## on 2 11 ## insane 6 9 ## by 2 9 ## monastery 9 4 ## relief 6 8 ## slope 5 11 ## scoundrel 9 5 ## with 4 8 ## neither 7 2 ## pretentious 11 4 ## solid 5 12 ## this 4 9 ## for 3 8 ## therefore 9 1 ## generality 10 4 ## arise 5 13 ## blot 4 15 ## infectious 10 6 3.1.1 Design of the data "]
]
