[
["preface.html", "Multivariate Cookbook Preface 0.1 Mulitvariate analyses vs. Univariate anlyses 0.2 Common methods in multivariate analysis", " Multivariate Cookbook Ju-Chi Yu 2019-11-17 Preface 0.1 Mulitvariate analyses vs. Univariate anlyses What are multivariate analyses? Component-based multivariate methods are primarily data driven exploratory techniques of the underlying structure (i.e. latent structure) of a multivariate data set. These underlying structers are represted by a mixture of the original variables—called components or factors—which explain the most information about your observations. When would I use multivariate instead of univariate analyses? 1. When you want to understand the relationship between multiple (2 +) outcome variables/measurements. Note a Principle Component Analysis (PCA) with two variables would be the same as a pearson correlation (r). Exploratory data analysis, when you don’t have a priori hypotheses about the structure of your data. Example: You are using a large archival data set that includes multiple behavioral measurements (i.e., surveys, performance data, etc). Dimention reduction Example: Machine learning Identify unique patterns across observations Example: Eigen faces (maybe), time series data, functional PCA -Univariate statistics are most appropriate to use when you have one outcome or measured variable and have planned inferences based on a priori hypotheses. 0.2 Common methods in multivariate analysis Insert table of commonly used mulitvariate analyses and the type of data needed to conduct said analysis Rows: one table, two table, and multi-table (2+) analyses for quantitative, qualitative, and mixed data sets Columns: Variable/data, Analysis, Results/output note we can use this table as a kind of index for people to find the type of analysis they should do based on the type of data they are wanting to analyses Table 0.1: Comparison between Multivariate and Univariate Analyses Type of Data Data Structure Analysis Results Quantitative Single Table PCA factor scores and column loadings Quantitative Two Tables PLS insert Quantitative Two Tables BADA insert Quantitative Multiple (2+) Tables MFA insert Quantitative Multiple (2+) TAbles Statis insert Qualitative Single Table CA insert Qualitative Two or More Tables MCA insert Qualitative Two or More Tables PLS-CA insert Mixed Two or More Tables Mixed PLS insert "],
["pca.html", "PCA", " PCA "],
["pca-intro.html", "1 What is Principal Component Analysis (PCA)?", " 1 What is Principal Component Analysis (PCA)? Multi-table techniques originate from the multivariate techniques that analyze a single table (i.e., PCA) and a pair of tables (i.e., PLSC, CCA and RDA). These techniques search for a linear combination (i.e., a mixture) of the original variables that gives the maximum amount of information. However, the information to be maximized is defined differently for different techniques, and, therefore, these techniques can be expressed as different optimization problems, which can usually be solved by a gradient descent algorithm that iteratively searches for the solution (i.e., the coefficients of this linear combination). Specifically, in each , the gradient descent algorithm generates a new set of numbers that gives more information about the data than the previously generated set of numbers, and this procedure repeats for several iterations until an optimum is reached. Interestingly, for most multivariate techniques, this optimum set of coefficients can also be found in one step by the SVD. The SVD is a maximization problem (i.e., an optimization problem that searches for a maximum) that works on an \\(\\times\\) rectangular matrix . The SVD searches for two vectors of coefficients denoted and which maximize the \\(\\delta\\) computed as: \\[\\begin{equation}\\label{Maximization_SVD} \\delta = \\argmax_{\\mathbf{p},\\mathbf{q}} \\left\\{\\mathbf{p\\transpose Xq}\\right\\}\\quad\\mathrm{under\\;the\\;constraints}\\quad\\mathbf{p\\transpose p = q\\transpose q =\\:}1. \\end{equation}\\] Here, (respectively ) is the (respectively ) that stores the coefficients to weight the rows (respectively the columns) of . This maximization problem can be solved by a gradient descent algorithm which consists of the following steps. First, a set of values is used to initialize the first left singular vector \\(\\prescript{}{}{\\mathbf{p}}^1\\), and these initialized (denoted by the subscript 0 on the left) values \\(\\prescript{}{0}{\\mathbf{p}}^1\\) are then used to compute the initial values of $ ^1 $ (denoted by \\(\\prescript{}{0}{\\mathbf{q}}^1\\)): \\[\\begin{equation}\\label{iter1_SVD} \\delta\\prescript{}{0}{\\mathbf{q}}^1\\mathbf{= X\\transpose}\\prescript{}{0}{\\mathbf{p}}^1. \\end{equation}\\] This $^1 $ can then be used to compute \\(\\prescript{}{1}{\\mathbf{p}}^1\\): \\[\\begin{equation}\\label{iter2_SVD} \\delta\\prescript{}{1}{\\mathbf{p}}^1\\mathbf{= X}\\prescript{}{0}{\\mathbf{q}}^1. \\end{equation}\\] Next, Equations and are repeated respectively with \\(\\prescript{}{1}{\\mathbf{p}}^1\\) and the next derived \\(\\prescript{}{1}{\\mathbf{q}}^1\\), and this procedure is iterated until $^1 $ and $ ^1 $ converge to stable outcomes. After the first set of singular vectors has been extracted, the second set (\\(\\textbf{p}^2\\) and \\(\\textbf{q}^2\\)) is derived from the data that are left (i.e., the ; \\(\\textbf{X} - \\delta\\mathbf{p}^1{\\mathbf{q}^1}\\transpose\\)) and is thus to the first set of singular vectors (i.e., $ {1}2 = 0 $). Following the extraction of the second set of singular vectors, the third set can be extracted from the residual of the first two singular vectors and so on. As a result, together with the constraints from Equation , the relationship between different sets of singular vectors can be written as: \\[\\begin{equation}\\label{MaximizationConstraints_SVD} {\\mathbf{p}^\\ell}\\transpose \\mathbf{p}^{\\ell^\\prime} = {\\mathbf{q}^\\ell}\\transpose \\mathbf{q}^{\\ell^\\prime} = \\begin{cases} \\quad 1, &amp; \\mathrm{when}\\;\\ell = \\ell^\\prime\\\\ \\quad 0, &amp; \\mathrm{when}\\;\\ell\\neq\\ell^\\prime \\end{cases}. \\end{equation}\\] If these multiple sets of singular vectors \\(\\textbf{p}^\\ell\\) and \\(\\textbf{q}^\\ell\\) are stored in the columns of matrices and : \\[\\begin{equation}\\label{buildPQ_SVD} \\mathbf{P} = \\left[\\mathbf{p}^1\\,|\\,\\mathbf{p}^2\\,|\\,...\\,|\\,\\mathbf{p}^\\ell|\\,...\\,|\\,\\mathbf{p}^\\textit{L}\\right] \\quad\\mathrm{and}\\quad \\mathbf{Q} = \\left[\\mathbf{q}^1\\,|\\,\\mathbf{q}^2\\,|\\,...\\,|\\,\\mathbf{q}^\\ell|\\,...\\,|\\,\\mathbf{q}^\\textit{L}\\right], \\end{equation}\\] then SVD (cf. Equation ) can be expressed as: \\[\\begin{equation} \\mathbf{X = P\\Delta Q\\transpose} \\label{PDQ} \\end{equation}\\] with the constraints rewritten from Equation as: \\[\\begin{equation} \\textbf{P}\\transpose\\textbf{P} = \\textbf{Q}\\transpose\\textbf{Q} = \\textbf{I}. \\label{PTPQTQequalI} \\end{equation}\\] Here, is the \\(\\times\\) matrix of the left singular vectors, is the \\(\\times\\) matrix of the right singular vectors, and \\(\\mathbf{\\Delta}\\) is the the \\(\\times\\) diagonal matrix of the singular values \\(\\delta\\)s. In addition, these $ $s are ordered from the largest to the smallest (i.e., \\(\\delta_1 &gt; \\delta_2 &gt;...&gt; \\delta_\\textit{L}\\)), and the total number of non-zero singular values is equal to the rank of . Thus, the SVD gives the optimal decomposition of with a given rank. Also, the squared singular values \\(\\mathbf{\\delta}^2\\) (or \\(\\mathbf{\\Delta}^2\\) as a diagonal matrix) are equal to the \\(\\mathbf{\\lambda}\\) (or \\(\\mathbf{\\Lambda}\\) as a diagonal matrix) of matrices \\(\\transpose\\) and \\(\\transpose\\), and and are the respective of \\(\\transpose\\) and \\(\\transpose\\). Note that when the eigenvalues of a matrix (e.g., ) are all positive, this matrix is called (pd); when the eigenvalues of this matrix are non-negative values, this matrix is called (psd). PCA extracts information by generating a set of orthogonal variables, called , with the first component explaining most of the variance of the data table. The elements of these components are called or . First, almost always in PCA, the data table is preprocessed by centering its columns (i.e., each column has a mean of zero). Next, the columns can be normalized to exclude their units and are therefore comparable to each other. Finally, the set of factor scores of the first component ($ ^1 $) is computed as a linear combination of the original variables: \\[\\begin{equation}\\label{f1_PCA} \\mathbf{f}^1 = \\mathbf{Xq}^1, \\end{equation}\\] where $ ^1 $ is a $ $ 1 right singular vector that stores the coefficients for each column of , and in PCA, these coefficients are called . Furthermore, because the first component is defined to have the largest variance, and because its variance is computed by the sum of squares of its factor scores: \\[\\begin{equation}\\label{f1var_PCA} {\\mathbf{f}^1}\\transpose\\mathbf{f}^1 = {\\mathbf{q}^1}\\transpose\\mathbf{X\\transpose} \\mathbf{Xq}^1, \\end{equation}\\] the step to find $ ^1 $ can then be described as the solution of the following maximization problem: \\[\\begin{equation}\\label{Maximization_PCA} \\begin{split} \\delta^2 &amp;= \\argmax_{\\mathbf{q}^1} \\left\\{{\\mathbf{f}^1}\\transpose\\mathbf{f}^1\\right\\} = \\argmax_{\\mathbf{q}^1} \\left\\{{\\mathbf{q}^1}\\transpose\\mathbf{X\\transpose} \\mathbf{Xq}^1\\right\\} \\\\ &amp;= \\argmax_{\\mathbf{q}^1}\\left\\{\\mathrm{var}(\\mathbf{Xq}^1)\\right\\} = \\argmax_{\\mathbf{q}^1} \\left\\{\\mathrm{cov}(\\mathbf{Xq}^1, \\mathbf{Xq}^1)\\right\\}\\quad \\mathrm{where}\\quad{\\mathbf{q}^1}\\transpose \\mathbf{q}^1 = 1. \\\\ \\end{split} \\end{equation}\\] After the set of factor scores of the first component have been found, the set of factor scores of the following components are computed sequentially from the residual so that these components are orthogonal. If the factor scores $ ^$ and loadings $ ^$ of each component are stored in different columns of matrices and such that \\[\\begin{equation}\\label{buildFQ_PCA} \\mathbf{F} = \\left[\\mathbf{f}^1\\,|\\,\\mathbf{f}^2\\,|\\,...\\,|\\,\\mathbf{f}^\\ell|\\,...\\,|\\,\\mathbf{f}^\\textit{L}\\right] \\quad\\mathrm{and}\\quad \\mathbf{Q} = \\left[\\mathbf{q}^1\\,|\\,\\mathbf{q}^2\\,|\\,...\\,|\\,\\mathbf{q}^\\ell|\\,...\\,|\\,\\mathbf{q}^\\textit{L}\\right], \\end{equation}\\] Equation can be rewritten as: \\[\\begin{equation} \\mathbf{F = XQ} \\quad\\mathrm{under\\;the\\;constraint\\;that}\\quad \\mathbf{Q\\transpose Q = I}. \\label{FactorScores} \\end{equation}\\] By computing the factor scores, PCA onto the components; in other words, the factor scores $ ^$ are the of the rows of onto the $ $th component. In PCA, the maximization problem can also be rewritten from Equation as the SVD of (recall Equation ): \\[\\begin{equation} \\mathbf{X = FQ\\transpose = P\\Delta Q\\transpose}\\quad\\mathrm{under\\;the\\;constraints}\\quad \\mathbf{P\\transpose P = Q\\transpose Q = I}. \\label{XequalFQ} \\end{equation}\\] In Equations and , (respectively ) is the (respectively ) \\(\\times\\) matrix of the left (respectively right) singular vectors, and \\(\\mathbf{\\Delta}\\) is the \\(\\times\\) diagonal matrix of the singular values \\(\\delta\\)s. Moreover, if we rewrite Equation from Equation , the factor scores can also be computed from and \\(\\boldsymbol{\\Delta}\\) as: \\[\\begin{equation} \\mathbf{F = XQ = P\\Delta} \\quad\\mathrm{under\\;the\\;constraints}\\quad \\mathbf{P\\transpose P = Q\\transpose Q = I}, \\label{FactorScores2} \\end{equation}\\] and the sums of squares of that are maximized in PCA are equal to the eigenvalues \\(\\mathbf{\\Lambda}\\) because \\[\\begin{equation}\\label{FtFeqLambda} \\mathbf{F\\transpose F = \\Delta P\\transpose P\\Delta = \\Delta}^2 = \\mathbf{\\Lambda}. \\end{equation}\\] As a result, each eigenvalue is the sum of squares of each component, and the sum of all eigenvalues gives the total sum of squares of the table—a quantity called the of the data table. When the eigenvalues are divided by \\(\\textit{I}-1\\), they give the variance of the components. Because the singular values are the square roots of the eigenvalues, singular values divided by \\(\\sqrt[]{\\textit{I}-1}\\) give the standard deviation of the components. In addition, Equation can be applied to map an out-of-sample observation (e.g., \\(\\mathbf{r}_\\mathrm{sup}\\) as a row with matching columns) or an out-of-sample variable (e.g., \\(\\mathbf{c}_\\mathrm{sup}\\) as a column with matching rows) onto the components. These out-of-sample elements are called elements—as opposed to the elements that are originally analyzed by PCA—and need to be preprocessed (i.e., centered and normalized) in the same way as the active elements of the original PCA. After proper preprocessing steps, the supplementary factor scores and loadings are computed as \\[\\begin{equation} \\mathbf{f}_\\mathrm{sup} = \\mathbf{r}_\\mathrm{sup} \\mathbf{Q}\\qquad \\mathrm{and}\\qquad \\mathbf{q}_\\mathrm{sup} = \\mathbf{c}_\\mathrm{sup} \\mathbf{P\\Delta}^{-1}. \\label{supp_PCA} \\end{equation}\\] "],
["svd.html", "2 Singular value decomposition", " 2 Singular value decomposition "],
["pca-example.html", "3 PCA example 3.1 Example data", " 3 PCA example 3.1 Example data library(ExPosition) ## Loading required package: prettyGraphs data(words) words$data ## length definitions ## bag 3 14 ## across 6 7 ## on 2 11 ## insane 6 9 ## by 2 9 ## monastery 9 4 ## relief 6 8 ## slope 5 11 ## scoundrel 9 5 ## with 4 8 ## neither 7 2 ## pretentious 11 4 ## solid 5 12 ## this 4 9 ## for 3 8 ## therefore 9 1 ## generality 10 4 ## arise 5 13 ## blot 4 15 ## infectious 10 6 3.1.1 Design of the data "]
]
