[
["preface.html", "Multivariate Cookbook Preface 0.1 Mulitvariate analyses vs. Univariate anlyses 0.2 Common methods in multivariate analysis", " Multivariate Cookbook Ju-Chi Yu and Sheila Meldrum 2019-11-17 Preface 0.1 Mulitvariate analyses vs. Univariate anlyses What are multivariate analyses? Component-based multivariate methods are primarily data driven exploratory techniques of the underlying structure (i.e. latent structure) of a multivariate data set. These underlying structers are represted by a mixture of the original variables—called components or factors—which explain the most information about your observations. When would I use multivariate instead of univariate analyses? 1. When you want to understand the relationship between multiple (2 +) outcome variables/measurements. Note a Principle Component Analysis (PCA) with two variables would be the same as a pearson correlation (r). Exploratory data analysis, when you don’t have a priori hypotheses about the structure of your data. Example: You are using a large archival data set that includes multiple behavioral measurements (i.e., surveys, performance data, etc). Dimention reduction Example: Machine learning Identify unique patterns across observations Example: Eigen faces (maybe), time series data, functional PCA -Univariate statistics are most appropriate to use when you have one outcome or measured variable and have planned inferences based on a priori hypotheses. 0.2 Common methods in multivariate analysis Insert table of commonly used mulitvariate analyses and the type of data needed to conduct said analysis Rows: one table, two table, and multi-table (2+) analyses for quantitative, qualitative, and mixed data sets Columns: Variable/data, Analysis, Results/output note we can use this table as a kind of index for people to find the type of analysis they should do based on the type of data they are wanting to analyses Table 0.1: Comparison between Multivariate and Univariate Analyses Type of Data Data Structure Analysis Results Quantitative Single Table PCA factor scores and column loadings Quantitative Two Tables PLS insert Quantitative Two Tables BADA insert Quantitative Multiple (2+) Tables MFA insert Quantitative Multiple (2+) TAbles Statis insert Qualitative Single Table CA insert Qualitative Two or More Tables MCA insert Qualitative Two or More Tables PLS-CA insert Mixed Two or More Tables Mixed PLS insert "],
["principal-component-analysis-pca.html", "Principal Component Analysis (PCA)", " Principal Component Analysis (PCA) "],
["pca-intro.html", "1 PCA introduction? 1.1 When should you do a PCA? 1.2 How PCA is different from factor analysis and item analysis? 1.3 What does PCA do? 1.4 What are expected from PCA? 1.5 How do I do PCA? 1.6 Show me the math", " 1 PCA introduction? 1.1 When should you do a PCA? 1.2 How PCA is different from factor analysis and item analysis? 1.3 What does PCA do? Principal component analysis (PCA) is used to analyze one table of quantitative data. PCA mixes the input variables to give new variables, called principal components (or factors, or latent variables). The first principal component is the best fit of line that maximizes the inertia—variance in 3-D (and beyond) space—of the cloud of data points. Subsequent components are defined as orthogonal to previous components, and maximize the remaining inertia. PCA gives one map for the rows (called factor scores), and one map for the columns (called loadings). These 2 maps are related, because they both are described by the same components. However, these 2 maps project different kinds of information onto the components, and so they are interpreted differently. Factor scores are the coordinates of the row observations. They are interpreted by the distances between them, and their distance from the origin. Loadings describe the column variables. Loadings are interpreted by the angle between them, and their distance from the origin. The distance from the origin is important in both maps, because squared distance from the mean is inertia (variance, information; see sum of squares as in ANOVA/regression). Because of the Pythagorean Theorem, the total information contributed by a data point (its squared distance to the origin) is also equal to the sum of its squared factor scores. 1.4 What are expected from PCA? 1.5 How do I do PCA? 1.6 Show me the math "],
["svd.html", "2 Singular value decomposition", " 2 Singular value decomposition "],
["pca-example.html", "3 PCA example 3.1 Example data", " 3 PCA example 3.1 Example data library(ExPosition) ## Loading required package: prettyGraphs data(words) words$data ## length definitions ## bag 3 14 ## across 6 7 ## on 2 11 ## insane 6 9 ## by 2 9 ## monastery 9 4 ## relief 6 8 ## slope 5 11 ## scoundrel 9 5 ## with 4 8 ## neither 7 2 ## pretentious 11 4 ## solid 5 12 ## this 4 9 ## for 3 8 ## therefore 9 1 ## generality 10 4 ## arise 5 13 ## blot 4 15 ## infectious 10 6 3.1.1 Design of the data "]
]
