[
["preface.html", "Multivariate Cookbook Preface What are Multivariate Analyses? Multivariate vs. Univariate Analyses Common Multivariate Analyses What Will this Book Cover? How Will this Book be Covered?", " Multivariate Cookbook Ju-Chi Yu and Sheila Meldrum 2019-12-15 Preface Multivariate techniques are becoming an increasingly popular analytical tool given the rise of both open and large datasets. This book hopes to provide conceptual and practical knowledge on how to perform these multivariate techniques. What are Multivariate Analyses? Multivariate analyses are used when there is more than one outcome (or dependent variables; DVs) as opposed to univariate analyses (e.g., ANOVA or regression) that have only one outcome. Component-based multivariate analyses are primarily data driven exploratory techniques of the underlying structure (i.e. latent structure) of a multivariate data set. These underlying structures are represented by a mixture of the original variables—called components or factors—which explains the most information about the observations. Multivariate vs. Univariate Analyses When to Use Univariate Analyses? Univariate statistics statistics are most appropriate to use when you have one outcome (or dependent variable) and have planned inferences based on a priori hypotheses. When to Use Multivariate Techniques? Understand the relationship between two or more outcome variables/measurements. Note: A Principle Component Analysis (PCA) with two variables would be the same as a pearson correlation (\\(r\\)) if the variables are normalized (i.e., z-scored) first. Exploratory data analysis When you don’t have a priori hypotheses about the structure of your data. Example: You are using a large archival data set that includes multiple behavioral measurements (i.e., surveys, performance data, etc). Dimension reduction Example: Machine learning Identify unique patterns across observations Example: Eigen faces (maybe), time series data, functional PCA Common Multivariate Analyses Type of Data No. of Tables Analysis Results Quantitative 1 PCA Factor Scores, Column Loadings Quantitative 2 PLS insert Quantitative 2 BADA insert Quantitative 2+ MFA insert Quantitative 2+ STATIS insert Qualitative 1 CA insert Qualitative 2+ MCA insert Qualitative 2+ PLS-CA insert Mixed 2+ Mixed PLS insert What Will this Book Cover? Currently, this book will cover PCA with other multivariate analyses being added over time. How Will this Book be Covered? Each multivariate technique will cover: A conceptual description A practical example of the technique using R and the ExPosition library What ExPosition is actually doing behing the scenes using R A walk-through of the math For the practical example of each technique, we will cover: Pre-processing of the data Using descriptive statistics to describe the data Performining the multivariate technique Determining the number of components Visualizing and interpreting the components Hopefully, this format will allow individuals that are new to multivariate techniques to grasp both conceptual and pratical information of each technique while allowing more experienced individuals to use this book as a quick reference. "],
["principal-component-analysis-pca.html", "1 Principal Component Analysis (PCA) 1.1 Description 1.2 Example 1.3 Behind-the-Scenes 1.4 Math Walk-Through", " 1 Principal Component Analysis (PCA) 1.1 Description 1.1.1 What is PCA? Principal component analysis (PCA) is used to analyze one table of quantitative data. PCA creates new variables, which are called principal components (or factors, or latent variables) from the input variables. The first principal component is the best fit of line that explains the most amount of variability across all of the variables. Subsequent components are defined as orthogonal to previous components, and explains the next most amount of variability across all of the variables. 1.1.2 Singular Value Decomposition (SVD) PCA creates these principal components using singular value decompisition (SVD). SVD is one of several decomposition methods (e.g., [insert other methods here]). SVD decomposes the data into three matrices (or maps). SVD gives one map for the rows of the table called factor scores and one map for the columns of the table called loadings. These two maps are related because they both are described by the same principal components. The third map gives the error between each component’s predicted data point and the actual data point called singular values. 1.1.3 How to Interpret these Matrices? However, these 2 maps project different kinds of information onto the components, and so they are interpreted differently. Factor scores are the coordinates of the row observations. They are interpreted by the distances between them, and their distance from the origin. Loadings describe the column variables. Loadings are interpreted by the angle between them, and their distance from the origin. The distance from the origin is important in both maps, because squared distance from the mean is inertia (variance, information; see sum of squares as in ANOVA/regression). Because of the Pythagorean Theorem, the total information contributed by a data point (its squared distance to the origin) is also equal to the sum of its squared factor scores. 1.1.4 What are the Assumptions of PCA? 1.2 Example Let’s say we are interested in creating unique components from a dataset that contains the length and number of definitions of a list of 20 words. 1.2.1 Packages First, let’s load the package ExPosition that contains both the data and a function to perform PCA. library(ExPosition) 1.2.2 Data data(words) # load words data to the environment data &lt;- words$data # set the words data as data data # view data ## length definitions ## bag 3 14 ## across 6 7 ## on 2 11 ## insane 6 9 ## by 2 9 ## monastery 9 4 ## relief 6 8 ## slope 5 11 ## scoundrel 9 5 ## with 4 8 ## neither 7 2 ## pretentious 11 4 ## solid 5 12 ## this 4 9 ## for 3 8 ## therefore 9 1 ## generality 10 4 ## arise 5 13 ## blot 4 15 ## infectious 10 6 This single table contains 20 rows of words and 2 columns that contain information about the length and number of definitions of each word. 1.2.3 Data Pre-Processing 1.2.4 Descriptive Statistics psych::describe(data) %&gt;% select(n, mean, median, min, max, sd, se, skew, kurtosis) ## n mean median min max sd se skew kurtosis ## length 20 6 5.5 2 11 2.81 0.63 0.28 -1.31 ## definitions 20 8 8.0 1 15 3.93 0.88 0.01 -1.06 cor(data) ## length definitions ## length 1.0000000 -0.7333333 ## definitions -0.7333333 1.0000000 1.2.5 Analysis To perform PCA, we can use the epPCA() function. model &lt;- epPCA(data, graphs = F) model$ExPosition.Data ## **Results for Principal Component Analysis** ## The analysis was performed on 20 individuals, described by 2 variables ## *The results are available in the following objects: ## ## name description ## 1 &quot;$fi&quot; &quot;Factor scores of the rows&quot; ## 2 &quot;$di&quot; &quot;Squared distances of the rows&quot; ## 3 &quot;$ci&quot; &quot;Contributions of the rows&quot; ## 4 &quot;$ri&quot; &quot;Cosines of the rows&quot; ## 5 &quot;$fj&quot; &quot;Factor scores of the columns&quot; ## 6 &quot;$dj&quot; &quot;square distances of the columns&quot; ## 7 &quot;$cj&quot; &quot;Contributions for the columns&quot; ## 8 &quot;$rj&quot; &quot;Cosines of the columns&quot; ## 9 &quot;$t&quot; &quot;Explained Variance&quot; ## 10 &quot;$eigs&quot; &quot;Eigenvalues&quot; ## 11 &quot;$pdq&quot; &quot;SVD data&quot; ## 12 &quot;$X&quot; &quot;X matrix to decompose&quot; ## 13 &quot;$M&quot; &quot;Masses - each set to 1&quot; ## 14 &quot;$W&quot; &quot;Weights - each set to 1&quot; ## 15 &quot;$center&quot; &quot;Center of X&quot; ## 16 &quot;$scale&quot; &quot;Scale factor of X&quot; 1.2.6 Number of Components 1.2.7 Visualize and Interpret Components 1.2.7.1 Factor Scores 1.2.7.2 Loadings 1.3 Behind-the-Scenes The single table is first decomposed using the svd() function. 1.3.1 SVD model &lt;- svd(data) 1.3.2 SVD Results The decomposition of the single table using SVD results in the three matrices: Singular values Left Singular Right Singular 1.3.2.1 Singular Values d &lt;- model$d d ## [1] 45.84224 18.50645 1.3.2.2 Left Singular u &lt;- model$u u ## [,1] [,2] ## [1,] -0.2913695 0.278633409 ## [2,] -0.1994408 -0.064140391 ## [3,] -0.2246669 0.235073639 ## [4,] -0.2359454 -0.004958752 ## [5,] -0.1881623 0.175892000 ## [6,] -0.1805212 -0.288550913 ## [7,] -0.2176931 -0.034549571 ## [8,] -0.2605042 0.099435575 ## [9,] -0.1987735 -0.258960094 ## [10,] -0.1938015 0.055875804 ## [11,] -0.1201250 -0.257307176 ## [12,] -0.2044127 -0.378976289 ## [13,] -0.2787565 0.129026394 ## [14,] -0.2120538 0.085466624 ## [15,] -0.1818557 0.101088492 ## [16,] -0.1257643 -0.377323372 ## [17,] -0.1924669 -0.333763601 ## [18,] -0.2970088 0.158617214 ## [19,] -0.3215676 0.263011541 ## [20,] -0.2289716 -0.274581962 1.3.2.3 Right Singular v &lt;- model$v v ## [,1] [,2] ## [1,] -0.5476211 -0.8367264 ## [2,] -0.8367264 0.5476211 1.3.3 Projections 1.3.3.1 Inertia 1.3.3.2 Factor Scores 1.3.3.3 Loadings 1.4 Math Walk-Through "],
["correspondence-analysis-ca.html", "2 Correspondence Analysis (CA) 2.1 Description 2.2 Example 2.3 Behind-the-Scenes 2.4 Math Walk-Through", " 2 Correspondence Analysis (CA) 2.1 Description 2.2 Example 2.3 Behind-the-Scenes 2.4 Math Walk-Through "]
]
