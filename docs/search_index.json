[
["preface.html", "Multivariate Cookbook Preface What are Multivariate Analyses? Multivariate vs. Univariate Analyses Common Multivariate Analyses What Will this Book Cover? How Will this Book be Covered?", " Multivariate Cookbook Ju-Chi Yu and Sheila Meldrum 2020-02-04 Preface Multivariate techniques are becoming an increasingly popular analytical tool given the rise of both open and large datasets. This book hopes to provide conceptual and practical knowledge on how to perform these multivariate techniques. What are Multivariate Analyses? Multivariate analyses are used when there is more than one outcome (or dependent variables; DVs) as opposed to univariate analyses (e.g., ANOVA or regression) that have only one outcome. Component-based multivariate analyses are primarily data driven exploratory techniques of the underlying structure (i.e. latent structure) of a multivariate data set. These underlying structures are represented by a mixture of the original variables—called components or factors—which explains the most information about the observations. Multivariate vs. Univariate Analyses When to Use Univariate Analyses? Univariate statistics statistics are most appropriate to use when you have one outcome (or dependent variable) and have planned inferences based on a priori hypotheses. When to Use Multivariate Techniques? Understand the relationship between two or more outcome variables/measurements. Note: A Principle Component Analysis (PCA) with two variables would be the same as a pearson correlation (\\(r\\)) if the variables are normalized (i.e., z-scored) first. Exploratory data analysis When you don’t have a priori hypotheses about the structure of your data. Example: You are using a large archival data set that includes multiple behavioral measurements (i.e., surveys, performance data, etc). Dimension reduction Example: Machine learning Identify unique patterns across observations Example: Eigen faces (maybe), time series data, functional PCA Common Multivariate Analyses Type of Data No. of Tables Analysis Results Quantitative 1 PCA Factor Scores, Column Loadings Quantitative 2 PLS insert Quantitative 2 BADA insert Quantitative 2+ MFA insert Quantitative 2+ STATIS insert Qualitative 1 CA insert Qualitative 2+ MCA insert Qualitative 2+ PLS-CA insert Mixed 2+ Mixed PLS insert What Will this Book Cover? Currently, this book will cover PCA with other multivariate analyses being added over time. How Will this Book be Covered? Each multivariate technique will cover: A conceptual description A practical example of the technique using R and the ExPosition library What ExPosition is actually doing behing the scenes using R A walk-through of the math For the practical example of each technique, we will cover: Pre-processing of the data Using descriptive statistics to describe the data Performining the multivariate technique Determining the number of components Visualizing and interpreting the components Hopefully, this format will allow individuals that are new to multivariate techniques to grasp both conceptual and pratical information of each technique while allowing more experienced individuals to use this book as a quick reference. "],
["packages.html", "Chapter 1 Packages 1.1 Functions", " Chapter 1 Packages Let’s first load some packages that we will use. library(tidyverse) # data wrangling, visualization library(ExPosition) # package to perform one table analyses (e.g., PCA and CCA) library(TExPosition) # package to perform two talbe analyses (e.g., PLS and PLSCA) library(furrr) library(ggpubr) library(tidytext) library(corrr) 1.1 Functions 1.1.1 plot_eig() plot_eig &lt;- function(eig) { data &lt;- tibble(eig) %&gt;% mutate(n_components = row_number()) ggplot(data, aes(n_components, eig)) + geom_line() + geom_point() + labs(x = &quot;No. of Components&quot;, y = &quot;Eigenvalue&quot;) + theme_classic() + scale_x_continuous(breaks = seq(1, nrow(data), 1)) } 1.1.2 plot_pairs_all() plot_pairs_all &lt;- function(data) { data &lt;- as.data.frame(data) n_comp &lt;- ncol(data) data_fig &lt;- tibble(x = 1:n_comp) %&gt;% mutate(y = list(1:n_comp)) %&gt;% unnest() %&gt;% mutate(pair = future_map2_chr(x, y, function(x, y) paste0(sort(c(x, y)), collapse = &quot;-&quot;)), duplicate = duplicated(pair)) %&gt;% filter(x != y, duplicate == FALSE) %&gt;% select(-pair, -duplicate) %&gt;% mutate(data_x = future_map(x, function(x) data[,x])) %&gt;% unnest() %&gt;% mutate(data_y = future_map(y, function(x) data[,x])) %&gt;% unnest() ggplot(data_fig, aes(data_x, data_y)) + geom_hline(yintercept = 0) + geom_vline(xintercept = 0) + geom_point() + theme_minimal() + labs(x = NULL, y = NULL) + #coord_fixed() + facet_grid(y ~ x, switch = &quot;both&quot;, scales = &quot;free&quot;) } 1.1.3 plot_contributions_all() plot_contributions_all &lt;- function(c, f) { contributions_signed &lt;- sign(f) * c %&gt;% as.data.frame() max &lt;- abs(c) %&gt;% max() rnames &lt;- row.names(contributions_signed) contributions_signed &lt;- contributions_signed %&gt;% as_tibble() %&gt;% mutate(var_1 = rnames) %&gt;% gather(., &quot;var_2&quot;, &quot;value&quot;, -var_1) %&gt;% mutate(var_1 = as.factor(var_1), var_2 = as.factor(var_2)) %&gt;% arrange(value, var_2) %&gt;% mutate(idx = row_number()) ggplot(contributions_signed, aes(var_1, value, fill = value)) + geom_col(show.legend = FALSE) + facet_wrap( ~ var_2, scales = &quot;free&quot;) + labs(x = NULL, y = NULL) + theme_minimal() + scale_fill_distiller(palette = &quot;RdBu&quot;, limits = c(-max, max)) } 1.1.4 plot_latents_all() plot_latents_all &lt;- function(data_x, data_y) { data_x &lt;- model_pls$TExPosition.Data$lx data_y &lt;- model_pls$TExPosition.Data$ly data_x &lt;- as.data.frame(data_x) data_y &lt;- as.data.frame(data_y) data &lt;- tibble(n_components = 1:nrow(x)) %&gt;% mutate(data_x = future_map(n_components, function(x) data_x[, x])) fig &lt;- ggplot(data, aes(x, y)) + geom_point() + theme_classic() + facet_wrap(~ n_components) return(fig) } "],
["datasets.html", "Chapter 2 Datasets 2.1 Wine 2.2 UCLA", " Chapter 2 Datasets 2.1 Wine This wine data contains 6 wines (rows) that rated on different qualities (columns). data(&quot;pca.wine&quot;) data_pca_wine &lt;- pca.wine$data data_pca_wine ## Oak_Type E1_fruity E1_Woody E1_Coffee E2_Red_fruit E2_Roasted E2_Vanillin E2_Woody E3_Fruity ## W1 1 1 3 2 1 2 3 2 2 ## W2 2 2 2 1 2 1 2 1 2 ## W3 2 2 1 1 2 1 1 1 2 ## W4 2 2 1 1 2 1 1 1 1 ## W5 1 1 3 2 1 2 3 2 1 ## W6 1 1 2 2 1 2 2 2 1 ## E3_Butter E3_Woody ## W1 2 2 ## W2 1 1 ## W3 1 1 ## W4 1 1 ## W5 2 2 ## W6 2 2 summary(data_pca_wine) ## Oak_Type E1_fruity E1_Woody E1_Coffee E2_Red_fruit E2_Roasted ## Min. :1.0 Min. :1.0 Min. :1.00 Min. :1.0 Min. :1.0 Min. :1.0 ## 1st Qu.:1.0 1st Qu.:1.0 1st Qu.:1.25 1st Qu.:1.0 1st Qu.:1.0 1st Qu.:1.0 ## Median :1.5 Median :1.5 Median :2.00 Median :1.5 Median :1.5 Median :1.5 ## Mean :1.5 Mean :1.5 Mean :2.00 Mean :1.5 Mean :1.5 Mean :1.5 ## 3rd Qu.:2.0 3rd Qu.:2.0 3rd Qu.:2.75 3rd Qu.:2.0 3rd Qu.:2.0 3rd Qu.:2.0 ## Max. :2.0 Max. :2.0 Max. :3.00 Max. :2.0 Max. :2.0 Max. :2.0 ## E2_Vanillin E2_Woody E3_Fruity E3_Butter E3_Woody ## Min. :1.00 Min. :1.0 Min. :1.0 Min. :1.0 Min. :1.0 ## 1st Qu.:1.25 1st Qu.:1.0 1st Qu.:1.0 1st Qu.:1.0 1st Qu.:1.0 ## Median :2.00 Median :1.5 Median :1.5 Median :1.5 Median :1.5 ## Mean :2.00 Mean :1.5 Mean :1.5 Mean :1.5 Mean :1.5 ## 3rd Qu.:2.75 3rd Qu.:2.0 3rd Qu.:2.0 3rd Qu.:2.0 3rd Qu.:2.0 ## Max. :3.00 Max. :2.0 Max. :2.0 Max. :2.0 Max. :2.0 2.2 UCLA This open dataset from UCLA contains 600 individuals (rows) and their some personality measures and scores on standardized tests (7 columns). The personality measures include locus_of_control, self_concept, and motivation. The standardized tests include read, write, math, and science. data_ucla &lt;- read.csv(&quot;https://stats.idre.ucla.edu/stat/data/mmreg.csv&quot;) head(data_ucla) ## locus_of_control self_concept motivation read write math science female ## 1 -0.84 -0.24 1.00 54.8 64.5 44.5 52.6 1 ## 2 -0.38 -0.47 0.67 62.7 43.7 44.7 52.6 1 ## 3 0.89 0.59 0.67 60.6 56.7 70.5 58.0 0 ## 4 0.71 0.28 0.67 62.7 56.7 54.7 58.0 0 ## 5 -0.64 0.03 1.00 41.6 46.3 38.4 36.3 1 ## 6 1.11 0.90 0.33 62.7 64.5 61.4 58.0 1 summary(data_ucla) ## locus_of_control self_concept motivation read write ## Min. :-2.23000 Min. :-2.620000 Min. :0.0000 Min. :28.3 Min. :25.50 ## 1st Qu.:-0.37250 1st Qu.:-0.300000 1st Qu.:0.3300 1st Qu.:44.2 1st Qu.:44.30 ## Median : 0.21000 Median : 0.030000 Median :0.6700 Median :52.1 Median :54.10 ## Mean : 0.09653 Mean : 0.004917 Mean :0.6608 Mean :51.9 Mean :52.38 ## 3rd Qu.: 0.51000 3rd Qu.: 0.440000 3rd Qu.:1.0000 3rd Qu.:60.1 3rd Qu.:59.90 ## Max. : 1.36000 Max. : 1.190000 Max. :1.0000 Max. :76.0 Max. :67.10 ## math science female ## Min. :31.80 Min. :26.00 Min. :0.000 ## 1st Qu.:44.50 1st Qu.:44.40 1st Qu.:0.000 ## Median :51.30 Median :52.60 Median :1.000 ## Mean :51.85 Mean :51.76 Mean :0.545 ## 3rd Qu.:58.38 3rd Qu.:58.65 3rd Qu.:1.000 ## Max. :75.50 Max. :74.20 Max. :1.000 "],
["principal-component-analysis-pca.html", "Chapter 3 Principal Component Analysis (PCA) 3.1 Description 3.2 Example 3.3 Behind-the-Scenes 3.4 Math Walk-Through", " Chapter 3 Principal Component Analysis (PCA) 3.1 Description 3.1.1 What is PCA? Principal component analysis (PCA) is used to analyze one table of quantitative data. PCA creates new variables, which are called principal components (or factors, or latent variables) from the input variables. The first principal component is the best fit of line that explains the most amount of variability across all of the variables. Subsequent components are defined as orthogonal to previous components, and explains the next most amount of variability across all of the variables. 3.1.2 Singular Value Decomposition (SVD) PCA creates these principal components using singular value decompisition (SVD). SVD is one of several decomposition methods (e.g., [insert other methods here]). SVD decomposes the data into three matrices (or maps). SVD gives one map for the rows of the table called factor scores and one map for the columns of the table called loadings. These two maps are related because they both are described by the same principal components. The third map gives the error between each component’s predicted data point and the actual data point called singular values. 3.1.3 How to Interpret these Matrices? However, these 2 maps project different kinds of information onto the components, and so they are interpreted differently. Factor scores are the coordinates of the row observations. They are interpreted by the distances between them, and their distance from the origin. Loadings describe the column variables. Loadings are interpreted by the angle between them, and their distance from the origin. The distance from the origin is important in both maps, because squared distance from the mean is inertia (variance, information; see sum of squares as in ANOVA/regression). Because of the Pythagorean Theorem, the total information contributed by a data point (its squared distance to the origin) is also equal to the sum of its squared factor scores. 3.1.4 What are the Assumptions of PCA? 3.2 Example 3.2.1 Packages 3.2.2 Data Pre-Processing data &lt;- data_pca_wine 3.2.3 Descriptive Statistics psych::describe(data_pca_wine) %&gt;% select(n, mean, median, min, max, sd, se, skew, kurtosis) ## n mean median min max sd se skew kurtosis ## Oak_Type 6 1.5 1.5 1 2 0.55 0.22 0 -2.31 ## E1_fruity 6 1.5 1.5 1 2 0.55 0.22 0 -2.31 ## E1_Woody 6 2.0 2.0 1 3 0.89 0.37 0 -1.96 ## E1_Coffee 6 1.5 1.5 1 2 0.55 0.22 0 -2.31 ## E2_Red_fruit 6 1.5 1.5 1 2 0.55 0.22 0 -2.31 ## E2_Roasted 6 1.5 1.5 1 2 0.55 0.22 0 -2.31 ## E2_Vanillin 6 2.0 2.0 1 3 0.89 0.37 0 -1.96 ## E2_Woody 6 1.5 1.5 1 2 0.55 0.22 0 -2.31 ## E3_Fruity 6 1.5 1.5 1 2 0.55 0.22 0 -2.31 ## E3_Butter 6 1.5 1.5 1 2 0.55 0.22 0 -2.31 ## E3_Woody 6 1.5 1.5 1 2 0.55 0.22 0 -2.31 cor(data) ## Oak_Type E1_fruity E1_Woody E1_Coffee E2_Red_fruit E2_Roasted E2_Vanillin ## Oak_Type 1.0000000 1.0000000 -0.8164966 -1.0000000 1.0000000 -1.0000000 -0.8164966 ## E1_fruity 1.0000000 1.0000000 -0.8164966 -1.0000000 1.0000000 -1.0000000 -0.8164966 ## E1_Woody -0.8164966 -0.8164966 1.0000000 0.8164966 -0.8164966 0.8164966 1.0000000 ## E1_Coffee -1.0000000 -1.0000000 0.8164966 1.0000000 -1.0000000 1.0000000 0.8164966 ## E2_Red_fruit 1.0000000 1.0000000 -0.8164966 -1.0000000 1.0000000 -1.0000000 -0.8164966 ## E2_Roasted -1.0000000 -1.0000000 0.8164966 1.0000000 -1.0000000 1.0000000 0.8164966 ## E2_Vanillin -0.8164966 -0.8164966 1.0000000 0.8164966 -0.8164966 0.8164966 1.0000000 ## E2_Woody -1.0000000 -1.0000000 0.8164966 1.0000000 -1.0000000 1.0000000 0.8164966 ## E3_Fruity 0.3333333 0.3333333 0.0000000 -0.3333333 0.3333333 -0.3333333 0.0000000 ## E3_Butter -1.0000000 -1.0000000 0.8164966 1.0000000 -1.0000000 1.0000000 0.8164966 ## E3_Woody -1.0000000 -1.0000000 0.8164966 1.0000000 -1.0000000 1.0000000 0.8164966 ## E2_Woody E3_Fruity E3_Butter E3_Woody ## Oak_Type -1.0000000 0.3333333 -1.0000000 -1.0000000 ## E1_fruity -1.0000000 0.3333333 -1.0000000 -1.0000000 ## E1_Woody 0.8164966 0.0000000 0.8164966 0.8164966 ## E1_Coffee 1.0000000 -0.3333333 1.0000000 1.0000000 ## E2_Red_fruit -1.0000000 0.3333333 -1.0000000 -1.0000000 ## E2_Roasted 1.0000000 -0.3333333 1.0000000 1.0000000 ## E2_Vanillin 0.8164966 0.0000000 0.8164966 0.8164966 ## E2_Woody 1.0000000 -0.3333333 1.0000000 1.0000000 ## E3_Fruity -0.3333333 1.0000000 -0.3333333 -0.3333333 ## E3_Butter 1.0000000 -0.3333333 1.0000000 1.0000000 ## E3_Woody 1.0000000 -0.3333333 1.0000000 1.0000000 3.2.4 Analysis To perform PCA, we can use the epPCA() function. model &lt;- epPCA(data, graphs = F) model$ExPosition.Data ## **Results for Principal Component Analysis** ## The analysis was performed on 6 individuals, described by 11 variables ## *The results are available in the following objects: ## ## name description ## 1 &quot;$fi&quot; &quot;Factor scores of the rows&quot; ## 2 &quot;$di&quot; &quot;Squared distances of the rows&quot; ## 3 &quot;$ci&quot; &quot;Contributions of the rows&quot; ## 4 &quot;$ri&quot; &quot;Cosines of the rows&quot; ## 5 &quot;$fj&quot; &quot;Factor scores of the columns&quot; ## 6 &quot;$dj&quot; &quot;square distances of the columns&quot; ## 7 &quot;$cj&quot; &quot;Contributions for the columns&quot; ## 8 &quot;$rj&quot; &quot;Cosines of the columns&quot; ## 9 &quot;$t&quot; &quot;Explained Variance&quot; ## 10 &quot;$eigs&quot; &quot;Eigenvalues&quot; ## 11 &quot;$pdq&quot; &quot;SVD data&quot; ## 12 &quot;$X&quot; &quot;X matrix to decompose&quot; ## 13 &quot;$M&quot; &quot;Masses - each set to 1&quot; ## 14 &quot;$W&quot; &quot;Weights - each set to 1&quot; ## 15 &quot;$center&quot; &quot;Center of X&quot; ## 16 &quot;$scale&quot; &quot;Scale factor of X&quot; 3.2.5 Number of Components plot_eig(model$ExPosition.Data$eigs) 3.2.6 Visualize and Interpret Components 3.2.6.1 Factor Scores plot_pairs_all(model$ExPosition.Data$fi) ## Warning: `cols` is now required. ## Please use `cols = c(y)` ## Warning: `cols` is now required. ## Please use `cols = c(data_x)` ## Warning: `cols` is now required. ## Please use `cols = c(data_y)` 3.2.6.2 Loadings plot_pairs_all(model$ExPosition.Data$fj) ## Warning: `cols` is now required. ## Please use `cols = c(y)` ## Warning: `cols` is now required. ## Please use `cols = c(data_x)` ## Warning: `cols` is now required. ## Please use `cols = c(data_y)` 3.2.6.3 Contributions plot_contributions_all(model$ExPosition.Data$ci, model$ExPosition.Data$fi) pair_plot_all_contributions(model$ExPosition.Data$cj, model$ExPosition.Data$fj) 3.3 Behind-the-Scenes The single table is first decomposed using the svd() function. 3.3.1 SVD model &lt;- svd(data) 3.3.2 SVD Results The decomposition of the single table using SVD results in the three matrices: Singular values Left Singular Right Singular 3.3.2.1 Singular Values d &lt;- model$d d ## [1] 1.320000e+01 3.767286e+00 1.365628e+00 8.381807e-01 1.358330e-15 4.050210e-16 3.3.2.2 Left Singular u &lt;- model$u u ## [,1] [,2] [,3] [,4] [,5] [,6] ## [1,] -0.4968909 0.3350115 0.38368361 0.4004002 0.5674679 0.1063651 ## [2,] -0.3876599 -0.3954209 0.54332009 -0.2546214 -0.3758488 0.4382591 ## [3,] -0.3283876 -0.5315756 -0.07031995 0.5208750 -0.1916191 -0.5446241 ## [4,] -0.3075753 -0.4628432 -0.43965138 -0.4056438 0.5674679 0.1063651 ## [5,] -0.4760786 0.4037439 0.01435218 -0.5261186 -0.1916191 -0.5446241 ## [6,] -0.4168063 0.2675892 -0.59928786 0.2493778 -0.3758488 0.4382591 3.3.2.3 Right Singular v &lt;- model$v v ## [,1] [,2] [,3] [,4] [,5] [,6] ## [1,] -0.2603803 -0.4707195 -0.09852947 -0.1850688 8.077925e-01 -8.867369e-02 ## [2,] -0.2603803 -0.4707195 -0.09852947 -0.1850688 -3.484510e-01 6.200100e-01 ## [3,] -0.3911972 0.2564668 0.41900188 -0.3250030 -4.828355e-02 -4.019953e-01 ## [4,] -0.2881192 0.1653311 -0.27031923 0.1287652 -9.119654e-02 -4.200740e-02 ## [5,] -0.2603803 -0.4707195 -0.09852947 -0.1850688 -4.593415e-01 -5.313363e-01 ## [6,] -0.2881192 0.1653311 -0.27031923 0.1287652 2.279914e-02 1.050185e-02 ## [7,] -0.3911972 0.2564668 0.41900188 -0.3250030 4.828355e-02 4.019953e-01 ## [8,] -0.2881192 0.1653311 -0.27031923 0.1287652 2.279914e-02 1.050185e-02 ## [9,] -0.2747224 -0.2589345 0.50436917 0.7765901 -3.046352e-15 2.791460e-16 ## [10,] -0.2881192 0.1653311 -0.27031923 0.1287652 2.279914e-02 1.050185e-02 ## [11,] -0.2881192 0.1653311 -0.27031923 0.1287652 2.279914e-02 1.050185e-02 3.3.3 Projections 3.3.3.1 Inertia 3.3.3.2 Factor Scores 3.3.3.3 Loadings 3.4 Math Walk-Through "],
["correspondence-analysis-ca.html", "Chapter 4 Correspondence Analysis (CA) 4.1 Description 4.2 Example 4.3 Behind-the-Scenes 4.4 Math Walk-Through", " Chapter 4 Correspondence Analysis (CA) 4.1 Description 4.2 Example 4.3 Behind-the-Scenes 4.4 Math Walk-Through "],
["partial-least-squares-pls.html", "Chapter 5 Partial Least Squares (PLS) 5.1 Description 5.2 Example 5.3 Behind-the-Scenes 5.4 Math Walk-Through", " Chapter 5 Partial Least Squares (PLS) 5.1 Description 5.1.1 What is PLS? Partial Least Squares (PLS) is used to analyze two tables of quantitative. PLS is essentially “data wrangling” followed by PCA. Data wrangling can include centering and/or scaling of each column of each dataset and then the cross product of the two datasets. \\[SCP = X&#39;Y\\] where \\(SCP\\) is the sum of cross product, \\(X&#39;\\) is the transposed matrix of the first dataset, \\(Y\\) is the matrix of the second dataset. We can then obtain the latent variables by multiplying each dataset by its respective singular value. \\[ L_X = Xu \\] \\[ L_Y = Yv \\] The maximum possible number of latent variables is always the least number of columns of either table. 5.2 Example 5.2.1 Data Let’s use the UCLA dataset. data &lt;- data_ucla %&gt;% select(locus_of_control:science) head(data) ## locus_of_control self_concept motivation read write math science ## 1 -0.84 -0.24 1.00 54.8 64.5 44.5 52.6 ## 2 -0.38 -0.47 0.67 62.7 43.7 44.7 52.6 ## 3 0.89 0.59 0.67 60.6 56.7 70.5 58.0 ## 4 0.71 0.28 0.67 62.7 56.7 54.7 58.0 ## 5 -0.64 0.03 1.00 41.6 46.3 38.4 36.3 ## 6 1.11 0.90 0.33 62.7 64.5 61.4 58.0 5.2.1.1 Descriptive Statistics summary(data) ## locus_of_control self_concept motivation read write ## Min. :-2.23000 Min. :-2.620000 Min. :0.0000 Min. :28.3 Min. :25.50 ## 1st Qu.:-0.37250 1st Qu.:-0.300000 1st Qu.:0.3300 1st Qu.:44.2 1st Qu.:44.30 ## Median : 0.21000 Median : 0.030000 Median :0.6700 Median :52.1 Median :54.10 ## Mean : 0.09653 Mean : 0.004917 Mean :0.6608 Mean :51.9 Mean :52.38 ## 3rd Qu.: 0.51000 3rd Qu.: 0.440000 3rd Qu.:1.0000 3rd Qu.:60.1 3rd Qu.:59.90 ## Max. : 1.36000 Max. : 1.190000 Max. :1.0000 Max. :76.0 Max. :67.10 ## math science ## Min. :31.80 Min. :26.00 ## 1st Qu.:44.50 1st Qu.:44.40 ## Median :51.30 Median :52.60 ## Mean :51.85 Mean :51.76 ## 3rd Qu.:58.38 3rd Qu.:58.65 ## Max. :75.50 Max. :74.20 5.2.1.2 Correlation Matrix correlate(data) %&gt;% rplot(colours = rev(RColorBrewer::brewer.pal(9, &quot;RdBu&quot;))) + theme(axis.text.x = element_text(hjust = 1, angle = 45)) ## ## Correlation method: &#39;pearson&#39; ## Missing treated using: &#39;pairwise.complete.obs&#39; ## Don&#39;t know how to automatically pick scale for object of type noquote. Defaulting to continuous. We can see that the personality measures are weakly correlated with each other, while the standardized tests are highly correlated with each other. Also, we can see that Locus of Control is moderately correlated with each of the standardized tests. 5.2.1.3 Split Let’s split the data into a dataset of personality metrics (\\(X\\)) and another dataset of the standardized tests (\\(Y\\)). X &lt;- data %&gt;% select(locus_of_control, self_concept, motivation) %&gt;% as.matrix() head(X) ## locus_of_control self_concept motivation ## [1,] -0.84 -0.24 1.00 ## [2,] -0.38 -0.47 0.67 ## [3,] 0.89 0.59 0.67 ## [4,] 0.71 0.28 0.67 ## [5,] -0.64 0.03 1.00 ## [6,] 1.11 0.90 0.33 Y &lt;- data %&gt;% select(read, write, math, science) %&gt;% as.matrix() head(Y) ## read write math science ## [1,] 54.8 64.5 44.5 52.6 ## [2,] 62.7 43.7 44.7 52.6 ## [3,] 60.6 56.7 70.5 58.0 ## [4,] 62.7 56.7 54.7 58.0 ## [5,] 41.6 46.3 38.4 36.3 ## [6,] 62.7 64.5 61.4 58.0 5.2.2 Analysis model_pls &lt;- tepPLS(X, Y, scale1 = TRUE, scale2 = TRUE, graphs = F) 5.2.3 Eigenvalue eig &lt;- model_pls$TExPosition.Data$pdq$Dv^2 plot_eig(eig) 5.2.4 Factor Scores model_pls$TExPosition.Data$fi ## [,1] [,2] [,3] ## locus_of_control -417.77812 19.01029 2.663535 ## self_concept -59.30769 25.96956 -6.641973 ## motivation -236.51229 -40.09212 -3.039363 plot_pairs_all(model_pls$TExPosition.Data$fi) ## Warning: `cols` is now required. ## Please use `cols = c(y)` ## Warning: `cols` is now required. ## Please use `cols = c(data_x)` ## Warning: `cols` is now required. ## Please use `cols = c(data_y)` model_pls$TExPosition.Data$fj ## [,1] [,2] [,3] ## read -259.3939 2.7145460 -3.697377 ## write -261.5493 -33.3908501 4.156636 ## math -235.5306 -0.1749094 -3.881711 ## science -206.9447 38.9979374 3.798954 plot_pairs_all(model_pls$TExPosition.Data$fj) ## Warning: `cols` is now required. ## Please use `cols = c(y)` ## Warning: `cols` is now required. ## Please use `cols = c(data_x)` ## Warning: `cols` is now required. ## Please use `cols = c(data_y)` 5.2.5 Latent Scores model_pls$TExPosition.Data$lx %&gt;% head() ## [,1] [,2] [,3] ## [1,] 0.7654400 -1.4637130 -0.56896322 ## [2,] 0.6834717 -0.6237667 0.32105270 ## [3,] -1.1371420 0.8357669 -0.31337608 ## [4,] -0.8513383 0.5145176 -0.03000207 ## [5,] 0.4608174 -1.1600693 -0.79367985 ## [6,] -0.9894445 1.9526936 -0.18849546 model_pls$TExPosition.Data$ly %&gt;% head() ## [,1] [,2] [,3] ## [1,] -0.4841143 -0.7257935 0.96134526 ## [2,] 0.2425046 0.7043251 -0.56442103 ## [3,] -1.9410240 0.2379724 -0.84734860 ## [4,] -1.2353508 0.2546568 -0.10831869 ## [5,] 2.2621597 -0.8511297 0.08521605 ## [6,] -2.0154595 -0.2686033 -0.03488452 #plot_all_latents(model_pls$TExPosition.Data$lx, model_pls$TExPosition.Data$ly) 5.3 Behind-the-Scenes 5.3.1 Wrangling Let’s center and scale (i.e., z-score) each column in each dataset. X &lt;- apply(X, 2, scale) Y &lt;- apply(Y, 2, scale) summary(X) ## locus_of_control self_concept motivation ## Min. :-3.4710 Min. :-3.72058 Min. :-1.92815 ## 1st Qu.:-0.6998 1st Qu.:-0.43219 1st Qu.:-0.96529 ## Median : 0.1693 Median : 0.03555 Median : 0.02675 ## Mean : 0.0000 Mean : 0.00000 Mean : 0.00000 ## 3rd Qu.: 0.6169 3rd Qu.: 0.61669 3rd Qu.: 0.98960 ## Max. : 1.8850 Max. : 1.67975 Max. : 0.98960 summary(Y) ## read write math science ## Min. :-2.33613 Min. :-2.7641 Min. :-2.12953 Min. :-2.6543 ## 1st Qu.:-0.76233 1st Qu.:-0.8312 1st Qu.:-0.78058 1st Qu.:-0.7586 ## Median : 0.01961 Median : 0.1763 Median :-0.05831 Median : 0.0862 ## Mean : 0.00000 Mean : 0.0000 Mean : 0.00000 Mean : 0.0000 ## 3rd Qu.: 0.81146 3rd Qu.: 0.7727 3rd Qu.: 0.69317 3rd Qu.: 0.7095 ## Max. : 2.38525 Max. : 1.5129 Max. : 2.51213 Max. : 2.3116 5.3.2 Sum of Cross Products scp &lt;- crossprod(X, Y) # this is slightly faster than t(X) %*% Y scp ## read write math science ## locus_of_control 223.76546 214.96723 202.02413 194.45154 ## self_concept 36.33285 11.64969 32.10502 41.82597 ## motivation 126.15534 152.29466 116.81307 69.28602 5.3.3 PCA model_svd &lt;- svd(scp) d &lt;- model_svd$d u &lt;- model_svd$u v &lt;- model_svd$v eig &lt;- d^2 5.3.4 Eigenvalue eig ## [1] 233994.01871 2643.18734 60.44796 plot_eig(eig) 5.3.5 Factor Scores v %*% diag(d) ## [,1] [,2] [,3] ## [1,] 259.3939 2.7145460 -3.697377 ## [2,] 261.5493 -33.3908501 4.156636 ## [3,] 235.5306 -0.1749094 -3.881711 ## [4,] 206.9447 38.9979374 3.798954 u %*% diag(d) ## [,1] [,2] [,3] ## [1,] 417.77812 19.01029 2.663535 ## [2,] 59.30769 25.96956 -6.641973 ## [3,] 236.51229 -40.09212 -3.039363 5.3.6 Latent Scores X %*% u %&gt;% head() ## [,1] [,2] [,3] ## [1,] -0.7654400 -1.4637130 -0.56896322 ## [2,] -0.6834717 -0.6237667 0.32105270 ## [3,] 1.1371420 0.8357669 -0.31337608 ## [4,] 0.8513383 0.5145176 -0.03000207 ## [5,] -0.4608174 -1.1600693 -0.79367985 ## [6,] 0.9894445 1.9526936 -0.18849546 Y %*% v %&gt;% head() ## [,1] [,2] [,3] ## [1,] 0.4841143 -0.7257935 0.96134526 ## [2,] -0.2425046 0.7043251 -0.56442103 ## [3,] 1.9410240 0.2379724 -0.84734860 ## [4,] 1.2353508 0.2546568 -0.10831869 ## [5,] -2.2621597 -0.8511297 0.08521605 ## [6,] 2.0154595 -0.2686033 -0.03488452 5.4 Math Walk-Through "]
]
